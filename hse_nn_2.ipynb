{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Копия блокнота \"hse_nn_2.ipynb\"",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91NWEnhBb0bd"
      },
      "source": [
        "# Подготовка среды и данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NovB599mOf9T"
      },
      "source": [
        "!pip install torchmetrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjYl-M4Xb4qi"
      },
      "source": [
        "!wget -O positive.csv https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv?dl=0\n",
        "!wget -O negative.csv https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv?dl=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgiT9Xow1sd5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim\n",
        "from torchmetrics.functional import f1, precision, recall, accuracy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6WWZiab6rTw"
      },
      "source": [
        "pos_tweets = pd.read_csv('positive.csv', encoding='utf-8', sep=';', header=None, names=[0,1,2,'text','tone',5,6,7,8,9,10,11])\n",
        "neg_tweets = pd.read_csv('negative.csv', encoding='utf-8', sep=';', header=None, names=[0,1,2,'text','tone',5,6,7,8,9,10,11])\n",
        "neg_tweets['tone'] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIsHGfQdIY9o"
      },
      "source": [
        "all_tweets_data = pos_tweets.append(neg_tweets)\n",
        "tweets_data = shuffle(all_tweets_data[['text','tone']])[:100000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JYJtXCaERbb"
      },
      "source": [
        "train_sentences, val_sentences = train_test_split(tweets_data, test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rojKOL3WLp5E"
      },
      "source": [
        "def preprocess(text):\n",
        "    tokens = text.lower().split()\n",
        "    tokens = [token.strip(punctuation) for token in tokens]\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8t4nDCR1sd-"
      },
      "source": [
        "word_vocab = Counter()\n",
        "\n",
        "for text in tweets_data['text']:\n",
        "    word_vocab.update(preprocess(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wKHM_LU1seA"
      },
      "source": [
        "filtered_word_vocab = set()\n",
        "\n",
        "for word in word_vocab:\n",
        "    if word_vocab[word] > 2:\n",
        "        filtered_word_vocab.add(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEUhJv5N1seC"
      },
      "source": [
        "word2id = {'PAD':0}\n",
        "\n",
        "for word in filtered_word_vocab:\n",
        "    word2id[word] = len(word2id)\n",
        "\n",
        "id2word = {i:word for word, i in word2id.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrzM7MnCQeP_"
      },
      "source": [
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRxpfdmEmNvC"
      },
      "source": [
        "symbol_vocab = Counter()\n",
        "for text in tweets_data['text']:\n",
        "    for word in preprocess(text):\n",
        "        symbol_vocab.update(list(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USJkjfFtmNvU"
      },
      "source": [
        "filtered_symbol_vocab = set()\n",
        "\n",
        "for symbol in symbol_vocab:\n",
        "    if symbol_vocab[symbol] > 5:\n",
        "        filtered_symbol_vocab.add(symbol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-dHzz7_mNvY"
      },
      "source": [
        "symbol2id = {'PAD':0}\n",
        "\n",
        "for symbol in filtered_symbol_vocab:\n",
        "    symbol2id[symbol] = len(symbol2id)\n",
        "\n",
        "id2symbol = {i:symbol for symbol, i in symbol2id.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIqariVccCgT"
      },
      "source": [
        "texts = all_tweets_data.text.apply(preprocess).tolist()\n",
        "w2v = gensim.models.Word2Vec(texts, size=100, window=5, min_count=1)\n",
        "weights = np.zeros((len(word2id), 100))\n",
        "count = 0\n",
        "for word, i in word2id.items():\n",
        "    if word == 'PAD':\n",
        "        continue   \n",
        "    try:\n",
        "        weights[i] = w2v.wv[word]    \n",
        "    except KeyError:\n",
        "      count += 1\n",
        "\n",
        "      weights[i] = np.random.normal(0,0.1,100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obiXRWLt1OZJ"
      },
      "source": [
        "# Определение датасета"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TywUEwPsK1ke"
      },
      "source": [
        "# Правильно подготовленный класс Dataset для второй архитектуры\n",
        "class TweetsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, word2id, symbol2id, DEVICE):\n",
        "        self.dataset = dataset['text'].values\n",
        "        self.word2id = word2id\n",
        "        self.symbol2id = symbol2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = torch.Tensor(dataset['tone'].values)\n",
        "        self.device = DEVICE\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        tokens = self.preprocess(self.dataset[index])\n",
        "        word_ids = torch.LongTensor([self.word2id[token] for token in tokens if token in self.word2id]) \n",
        "        symbol_ids = torch.LongTensor([self.symbol2id[symbol] for token in tokens if token in self.word2id for symbol in token if symbol in self.symbol2id])\n",
        "        y = self.target[index]\n",
        "        return word_ids, symbol_ids, y\n",
        "    \n",
        "    def preprocess(self, text):\n",
        "        tokens = text.lower().split()\n",
        "        tokens = [token.strip(punctuation) for token in tokens]\n",
        "        tokens = [token for token in tokens if token]\n",
        "        return tokens\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        word_ids, symbol_ids, y = list(zip(*batch))\n",
        "        word_ids = pad_sequence(word_ids, batch_first=True).to(self.device)\n",
        "        symbol_ids = pad_sequence(symbol_ids, batch_first=True).to(self.device)\n",
        "        y = torch.Tensor(y).to(self.device)\n",
        "        return word_ids, symbol_ids, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Zl4FxB71seI"
      },
      "source": [
        "train_dataset = TweetsDataset(train_sentences, word2id, symbol2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZR_upb-_1seJ"
      },
      "source": [
        "val_dataset = TweetsDataset(val_sentences, word2id, symbol2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckE86zFb3eG4"
      },
      "source": [
        "# Функции для тренировки и оценки нейронной сети"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZT5iGBM3gif"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, accuracy_f, precision_f, recall_f):\n",
        "    print('Training...')\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    for i, (texts, symbols, ys) in enumerate(iterator):\n",
        "        optimizer.zero_grad()\n",
        "        preds_proba = model(texts, symbols).squeeze()\n",
        "        loss = criterion(preds_proba, ys) \n",
        "        loss.backward()  \n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        batch_accuracy = accuracy_f(preds_proba.round().long(), ys.long(), ignore_index=0)\n",
        "        epoch_accuracy += batch_accuracy\n",
        "        batch_precision = precision_f(preds_proba.round().long(), ys.long(), ignore_index=0)\n",
        "        epoch_precision += batch_precision\n",
        "        batch_recall = recall_f(preds_proba.round().long(), ys.long(), ignore_index=0)\n",
        "        epoch_recall += batch_recall\n",
        "\n",
        "        if not (i + 1) % 1000:\n",
        "            print(f'Train loss: {epoch_loss/i}, train accuracy: {epoch_accuracy/i}, train precision: {epoch_precision/i}, train recall: {epoch_recall/i},')\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_accuracy / len(iterator), epoch_precision / len(iterator), epoch_recall / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68GIKMny3gii"
      },
      "source": [
        "def evaluate(model, iterator, criterion, accuracy_f, precision_f, recall_f):\n",
        "    print(\"\\nValidating...\")\n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "\n",
        "    model.eval() \n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, (texts, symbols, ys) in enumerate(iterator):   \n",
        "            preds_proba = model(texts, symbols).squeeze() \n",
        "            loss = criterion(preds_proba, ys)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "            batch_accuracy = accuracy_f(preds_proba.round().long(), ys.long(), ignore_index=0)\n",
        "            epoch_accuracy += batch_accuracy\n",
        "            batch_precision = precision_f(preds_proba.round().long(), ys.long(), ignore_index=0)\n",
        "            epoch_precision += batch_precision\n",
        "            batch_recall = recall_f(preds_proba.round().long(), ys.long(), ignore_index=0)\n",
        "            epoch_recall += batch_recall\n",
        "\n",
        "            if not (i + 1) % 1000:\n",
        "              print(f'Val loss: {epoch_loss/i}, val accuracy: {epoch_accuracy/i}, val precision: {epoch_precision/i}, val recall: {epoch_recall/i},')\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_accuracy / len(iterator), epoch_precision / len(iterator), epoch_recall / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUCT8ayD1seK"
      },
      "source": [
        "# Определение архитектуры нейронной сети"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5q9a2i_8Nty"
      },
      "source": [
        "# Если при инициализации задать веса для слоя эмбеддинга, то он не будет обучаться. Иначе - будет\n",
        "class Net2(nn.Module):\n",
        "    def __init__(self, word_vocab_size, symbol_vocab_size, word_embedding_dim=180, symbol_embedding_dim=12, embedding_weights=None):\n",
        "        super().__init__()\n",
        "        if embedding_weights is not None:\n",
        "            word_embedding_dim = 100\n",
        "            self.embedding1 = nn.Embedding(word_vocab_size, word_embedding_dim)\n",
        "            self.embedding1.from_pretrained(torch.tensor(weights), freeze=True)\n",
        "        else:\n",
        "            word_embedding_dim = word_embedding_dim\n",
        "            self.embedding1 = nn.Embedding(word_vocab_size, word_embedding_dim)\n",
        "\n",
        "        bi_output = 64\n",
        "        tri_output = 32\n",
        "        x_len = 16\n",
        "\n",
        "        self.linear1 = nn.Linear(in_features=word_embedding_dim, out_features=x_len)\n",
        "\n",
        "\n",
        "        self.embedding2 = nn.Embedding(symbol_vocab_size, symbol_embedding_dim)\n",
        "        self.bigrams2 = nn.Conv1d(in_channels=symbol_embedding_dim, out_channels=bi_output, kernel_size=2, padding='same')\n",
        "        self.trigrams2 = nn.Conv1d(in_channels=symbol_embedding_dim, out_channels=tri_output, kernel_size=3, padding='same')        \n",
        "\n",
        "\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.hidden = nn.Linear(in_features=bi_output+tri_output+x_len, out_features=1)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word, symbol):\n",
        "        input1 = self.embedding1(word) # первый вход для эмбеддингов слов\n",
        "        input1 = torch.mean(input1, dim=1) # mean\n",
        "        X1 = self.linear1(input1) # линейный слой\n",
        "\n",
        "        input2 = self.embedding2(symbol) # второй вход для символьного представления слов\n",
        "        input2 = input2.transpose(1, 2)\n",
        "        feature_map_bigrams = self.dropout(self.pooling(self.relu(self.bigrams2(input2)))) # свёрточный слой с одинм размером окна\n",
        "        feature_map_trigrams = self.dropout(self.pooling(self.relu(self.trigrams2(input2)))) # свёрточный слой с другим размером окна\n",
        "        bi_pooling2 = feature_map_bigrams.max(2)[0] # max pooling over time\n",
        "        tri_pooling2 = feature_map_trigrams.max(2)[0] # max pooling over time\n",
        "\n",
        "        concat = torch.cat((X1, bi_pooling2, tri_pooling2), 1) # конкатенация\n",
        "        logits = self.hidden(concat) # линейный слой\n",
        "        logits = self.out(logits) # сигмоида\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH_4dqDbLtye"
      },
      "source": [
        "# Создание и тренировка нейронной сети"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DtgYWy3pwBF"
      },
      "source": [
        "# model = Net2(len(word2id), 5)\n",
        "model = Net2(len(word2id), len(symbol2id), embedding_weights=weights)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.BCELoss() # функция потерь BCELoss\n",
        "\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qMBLFzmpwBJ"
      },
      "source": [
        "# Вторая модель правильно построена и обучается\n",
        "losses = []\n",
        "losses_eval = []\n",
        "accurs = []\n",
        "accurs_eval = []\n",
        "precs = []\n",
        "precs_eval = []\n",
        "recs = []\n",
        "recs_eval = []\n",
        "\n",
        "for i in range(5):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    epoch_loss, epoch_accur, epoch_prec, epoch_rec = train(model, train_iterator, optimizer, criterion, accuracy, precision, recall)\n",
        "    losses.append(epoch_loss)\n",
        "    accurs.append(epoch_accur)\n",
        "    precs.append(epoch_prec)\n",
        "    recs.append(epoch_rec)\n",
        "\n",
        "    epoch_loss_on_test, epoch_accur_on_test, epoch_prec_on_test, epoch_rec_on_test = evaluate(model, val_iterator, criterion, accuracy, precision, recall)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    accurs_eval.append(epoch_accur_on_test)\n",
        "    precs_eval.append(epoch_prec_on_test)\n",
        "    recs_eval.append(epoch_rec_on_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OG56cVPOj47C"
      },
      "source": [
        "# Оценка качества обучения нейронной сети\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNki6wlImOfg"
      },
      "source": [
        "plt.plot(losses)\n",
        "plt.plot(losses_eval)\n",
        "plt.title('BCE loss value')\n",
        "plt.ylabel('BCE loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1GmrJ4PZnxy"
      },
      "source": [
        "plt.plot(accurs)\n",
        "plt.plot(accurs_eval)\n",
        "plt.title('Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlKCMKD7ZoeJ"
      },
      "source": [
        "plt.plot(precs)\n",
        "plt.plot(precs_eval)\n",
        "plt.title('Precision')\n",
        "plt.ylabel('Precision')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjC6NUu_mOY4"
      },
      "source": [
        "plt.plot(recs)\n",
        "plt.plot(recs_eval)\n",
        "plt.title('Recall')\n",
        "plt.ylabel('Recall')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehwxIg6EXFBI"
      },
      "source": [
        "# Усложненная архитектура нейронной сети"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah92gH7aZneB"
      },
      "source": [
        "# Если при инициализации задать веса для слоя эмбеддинга, то он не будет обучаться. Иначе - будет\n",
        "class Net2_1(nn.Module):\n",
        "    def __init__(self, word_vocab_size, symbol_vocab_size, word_embedding_dim=180, symbol_embedding_dim=12, embedding_weights=None):\n",
        "        super().__init__()\n",
        "        if embedding_weights is not None:\n",
        "            word_embedding_dim = 100\n",
        "            self.embedding1 = nn.Embedding(word_vocab_size, word_embedding_dim)\n",
        "            self.embedding1.from_pretrained(torch.tensor(weights), freeze=True)\n",
        "        else:\n",
        "            word_embedding_dim = word_embedding_dim\n",
        "            self.embedding1 = nn.Embedding(word_vocab_size, word_embedding_dim)\n",
        "\n",
        "        bi_output = 64\n",
        "        tri_output = 128\n",
        "        tetra_output = 256\n",
        "        x_len = 512\n",
        "\n",
        "        self.linear1 = nn.Linear(in_features=word_embedding_dim, out_features=x_len)\n",
        "\n",
        "\n",
        "        self.embedding2 = nn.Embedding(symbol_vocab_size, symbol_embedding_dim)\n",
        "        self.bigrams2 = nn.Conv1d(in_channels=symbol_embedding_dim, out_channels=bi_output, kernel_size=2, padding='same')\n",
        "        self.trigrams2 = nn.Conv1d(in_channels=symbol_embedding_dim, out_channels=tri_output, kernel_size=3, padding='same')        \n",
        "        self.tetragrams2 = nn.Conv1d(in_channels=symbol_embedding_dim, out_channels=tetra_output, kernel_size=4, padding='same') \n",
        "\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.hidden = nn.Linear(in_features=bi_output+tri_output+tetra_output+x_len, out_features=1)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word, symbol):\n",
        "        input1 = self.embedding1(word)\n",
        "        input1 = torch.mean(input1, dim=1)\n",
        "        X1 = self.linear1(input1)\n",
        "\n",
        "        input2 = self.embedding2(symbol)\n",
        "        input2 = input2.transpose(1, 2)\n",
        "        feature_map_bigrams = self.dropout(self.pooling(self.relu(self.bigrams2(input2))))\n",
        "        feature_map_trigrams = self.dropout(self.pooling(self.relu(self.trigrams2(input2))))\n",
        "        feature_map_tetragrams = self.dropout(self.pooling(self.relu(self.tetragrams2(input2))))\n",
        "        bi_pooling2 = feature_map_bigrams.max(2)[0]\n",
        "        tri_pooling2 = feature_map_trigrams.max(2)[0]\n",
        "        tetra_pooling2 = feature_map_tetragrams.max(2)[0]\n",
        "\n",
        "        concat = torch.cat((X1, bi_pooling2, tri_pooling2, tetra_pooling2), 1)\n",
        "        logits = self.hidden(concat)\n",
        "        logits = self.out(logits)\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtmHEi0MZneG"
      },
      "source": [
        "# model = Net2_1(len(word2id), 5)\n",
        "model = Net2_1(len(word2id), len(symbol2id), embedding_weights=weights)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.BCELoss() # функция потерь BCELoss\n",
        "\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdjEyWxsZneI"
      },
      "source": [
        "# Вторая модель правильно построена и обучается\n",
        "losses = []\n",
        "losses_eval = []\n",
        "accurs = []\n",
        "accurs_eval = []\n",
        "precs = []\n",
        "precs_eval = []\n",
        "recs = []\n",
        "recs_eval = []\n",
        "\n",
        "for i in range(5):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    epoch_loss, epoch_accur, epoch_prec, epoch_rec = train(model, train_iterator, optimizer, criterion, accuracy, precision, recall)\n",
        "    losses.append(epoch_loss)\n",
        "    accurs.append(epoch_accur)\n",
        "    precs.append(epoch_prec)\n",
        "    recs.append(epoch_rec)\n",
        "\n",
        "    epoch_loss_on_test, epoch_accur_on_test, epoch_prec_on_test, epoch_rec_on_test = evaluate(model, val_iterator, criterion, accuracy, precision, recall)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    accurs_eval.append(epoch_accur_on_test)\n",
        "    precs_eval.append(epoch_prec_on_test)\n",
        "    recs_eval.append(epoch_rec_on_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJHpxwQyZneM"
      },
      "source": [
        "plt.plot(losses)\n",
        "plt.plot(losses_eval)\n",
        "plt.title('BCE loss value')\n",
        "plt.ylabel('BCE loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGQV6zGQZneN"
      },
      "source": [
        "plt.plot(accurs)\n",
        "plt.plot(accurs_eval)\n",
        "plt.title('Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opU6ttKbZneP"
      },
      "source": [
        "plt.plot(precs)\n",
        "plt.plot(precs_eval)\n",
        "plt.title('Precision')\n",
        "plt.ylabel('Precision')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMZ2QUJmZneQ"
      },
      "source": [
        "plt.plot(recs)\n",
        "plt.plot(recs_eval)\n",
        "plt.title('Recall')\n",
        "plt.ylabel('Recall')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSJiAHWkoVLD"
      },
      "source": [
        "# Ещё одна усложненная архитектура нейронной сети"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD-MqA3YoOEd"
      },
      "source": [
        "# Если при инициализации задать веса для слоя эмбеддинга, то он не будет обучаться. Иначе - будет\n",
        "class Net2_2(nn.Module):\n",
        "    def __init__(self, word_vocab_size, symbol_vocab_size, word_embedding_dim=180, symbol_embedding_dim=12, embedding_weights=None):\n",
        "        super().__init__()\n",
        "        if embedding_weights is not None:\n",
        "            word_embedding_dim = 100\n",
        "            self.embedding1 = nn.Embedding(word_vocab_size, word_embedding_dim)\n",
        "            self.embedding1.from_pretrained(torch.tensor(weights), freeze=True)\n",
        "        else:\n",
        "            word_embedding_dim = word_embedding_dim\n",
        "            self.embedding1 = nn.Embedding(word_vocab_size, word_embedding_dim)\n",
        "\n",
        "        bi_output = 64\n",
        "        tri_output = 128\n",
        "        tetra_output = 256\n",
        "        x_len = 512\n",
        "        conv_output = 512\n",
        "        \n",
        "        self.linear1 = nn.Linear(in_features=word_embedding_dim, out_features=x_len)\n",
        "\n",
        "\n",
        "        self.embedding2 = nn.Embedding(symbol_vocab_size, symbol_embedding_dim)\n",
        "        self.bigrams1 = nn.Conv1d(in_channels=symbol_embedding_dim, out_channels=bi_output, kernel_size=2, padding='same')\n",
        "        self.trigrams1 = nn.Conv1d(in_channels=symbol_embedding_dim, out_channels=tri_output, kernel_size=3, padding='same')        \n",
        "        self.tetragrams1 = nn.Conv1d(in_channels=symbol_embedding_dim, out_channels=tetra_output, kernel_size=4, padding='same') \n",
        "        self.bigrams2 = nn.Conv1d(in_channels=conv_output, out_channels=bi_output, kernel_size=2, padding='same')\n",
        "        self.trigrams2 = nn.Conv1d(in_channels=conv_output, out_channels=tri_output, kernel_size=3, padding='same')  \n",
        "        self.tetragrams2 = nn.Conv1d(in_channels=conv_output, out_channels=tetra_output, kernel_size=4, padding='same')\n",
        "\n",
        "        self.conv = nn.Conv1d(in_channels=bi_output+tri_output+tetra_output, out_channels=conv_output, kernel_size=5, padding='same')\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.hidden = nn.Linear(in_features=bi_output+tri_output+tetra_output+x_len, out_features=1)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word, symbol):\n",
        "        input1 = self.embedding1(word)\n",
        "        input1 = torch.mean(input1, dim=1)\n",
        "        X1 = self.linear1(input1)\n",
        "\n",
        "        input2 = self.embedding2(symbol)\n",
        "        input2 = input2.transpose(1, 2)\n",
        "        feature_map_bigrams = self.relu(self.bigrams1(input2))\n",
        "        feature_map_trigrams = self.relu(self.trigrams1(input2))\n",
        "        feature_map_tetragrams = self.relu(self.tetragrams1(input2))\n",
        "        concat = torch.cat((feature_map_bigrams, feature_map_trigrams, feature_map_tetragrams), 1)\n",
        "\n",
        "        feature_map = self.dropout(self.pooling(self.relu(self.conv(concat))))\n",
        "\n",
        "        feature_map_bigrams = self.dropout(self.pooling(self.relu(self.bigrams2(feature_map))))\n",
        "        feature_map_trigrams = self.dropout(self.pooling(self.relu(self.trigrams2(feature_map))))\n",
        "        feature_map_tetragrams = self.dropout(self.pooling(self.relu(self.tetragrams2(feature_map))))\n",
        "        bi_pooling2 = feature_map_bigrams.max(2)[0]\n",
        "        tri_pooling2 = feature_map_trigrams.max(2)[0]\n",
        "        tetra_pooling2 = feature_map_tetragrams.max(2)[0]\n",
        "\n",
        "        concat = torch.cat((X1, bi_pooling2, tri_pooling2, tetra_pooling2), 1)\n",
        "        logits = self.hidden(concat)\n",
        "        logits = self.out(logits)\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnO3EqDkoOEv"
      },
      "source": [
        "# model = Net2_2(len(word2id), 5)\n",
        "model = Net2_2(len(word2id), len(symbol2id), embedding_weights=weights)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.BCELoss() # функция потерь BCELoss\n",
        "\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYG5nlZBoOEz"
      },
      "source": [
        "# Вторая модель правильно построена и обучается\n",
        "losses = []\n",
        "losses_eval = []\n",
        "accurs = []\n",
        "accurs_eval = []\n",
        "precs = []\n",
        "precs_eval = []\n",
        "recs = []\n",
        "recs_eval = []\n",
        "\n",
        "for i in range(5):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    epoch_loss, epoch_accur, epoch_prec, epoch_rec = train(model, train_iterator, optimizer, criterion, accuracy, precision, recall)\n",
        "    losses.append(epoch_loss)\n",
        "    accurs.append(epoch_accur)\n",
        "    precs.append(epoch_prec)\n",
        "    recs.append(epoch_rec)\n",
        "\n",
        "    epoch_loss_on_test, epoch_accur_on_test, epoch_prec_on_test, epoch_rec_on_test = evaluate(model, val_iterator, criterion, accuracy, precision, recall)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    accurs_eval.append(epoch_accur_on_test)\n",
        "    precs_eval.append(epoch_prec_on_test)\n",
        "    recs_eval.append(epoch_rec_on_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHPsud90oOE2"
      },
      "source": [
        "plt.plot(losses)\n",
        "plt.plot(losses_eval)\n",
        "plt.title('BCE loss value')\n",
        "plt.ylabel('BCE loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Hdnz32joOE3"
      },
      "source": [
        "plt.plot(accurs)\n",
        "plt.plot(accurs_eval)\n",
        "plt.title('Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NM35hfPYoOE4"
      },
      "source": [
        "plt.plot(precs)\n",
        "plt.plot(precs_eval)\n",
        "plt.title('Precision')\n",
        "plt.ylabel('Precision')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTlNlzgDoOE7"
      },
      "source": [
        "plt.plot(recs)\n",
        "plt.plot(recs_eval)\n",
        "plt.title('Recall')\n",
        "plt.ylabel('Recall')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E295bBkmkinH"
      },
      "source": [
        "# Анализа предсказаний "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lreo75C9bVby"
      },
      "source": [
        "def predict(model, iterator):\n",
        "    model.eval()\n",
        "    fp = []\n",
        "    fn = []\n",
        "    tp = [] \n",
        "    tn = []\n",
        "    with torch.no_grad():\n",
        "        for i, (texts, symbols, ys) in enumerate(iterator):   \n",
        "            preds = model(texts, symbols)\n",
        "\n",
        "            for pred, gold, text in zip(preds, ys, texts):              \n",
        "                text = ''.join([id2word[int(word)] for word in text if word !=0])\n",
        "                if round(pred.item()) > gold:\n",
        "                    fp.append(text)\n",
        "                elif round(pred.item()) < gold:\n",
        "                    fn.append(text)\n",
        "                elif round(pred.item()) == gold == 1:\n",
        "                    tp.append(text)\n",
        "                elif round(pred.item()) == gold == 0:\n",
        "                    tn.append(text)\n",
        "\n",
        "    return fp, fn, tp, tn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSc8EUKkenAk"
      },
      "source": [
        "predict(model, train_iterator)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}